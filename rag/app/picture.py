#
#  Copyright 2025 The InfiniFlow Authors. All Rights Reserved.
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
#

import io
import re

import numpy as np
from PIL import Image

from api.db import LLMType
from api.db.services.llm_service import LLMBundle
from deepdoc.vision import OCR
from rag.nlp import tokenize
from rag.utils import clean_markdown_block
from rag.nlp import rag_tokenizer


ocr = OCR()


def chunk(filename, binary, tenant_id, lang, callback=None, **kwargs):
    img = Image.open(io.BytesIO(binary)).convert("RGB")
    doc = {"docnm_kwd": filename, "title_tks": rag_tokenizer.tokenize(re.sub(r"\.[a-zA-Z]+$", "", filename)), "image": img, "doc_type_kwd": "image"}
    bxs = ocr(np.array(img))
    txt = "\n".join([t[0] for _, t in bxs if t[0]])
    eng = lang.lower() == "english"
    callback(0.4, "Finish OCR: (%s ...)" % txt[:12])
    if (eng and len(txt.split()) > 32) or len(txt) > 32:
        tokenize(doc, txt, eng)
        callback(0.8, "OCR results is too long to use CV LLM.")
        return [doc]

    try:
        callback(0.4, "Use CV LLM to describe the picture.")
        cv_mdl = LLMBundle(tenant_id, LLMType.IMAGE2TEXT, lang=lang)
        img_binary = io.BytesIO()
        img.save(img_binary, format="JPEG")
        img_binary.seek(0)
        ans = cv_mdl.describe(img_binary.read())
        callback(0.8, "CV LLM respond: %s ..." % ans[:32])
        txt += "\n" + ans
        tokenize(doc, txt, eng)
        return [doc]
    except Exception as e:
        callback(prog=-1, msg=str(e))

    return []


def vision_llm_chunk(binary, vision_model, prompt=None, callback=None):
    """
    A simple wrapper to process image to markdown texts via VLM.

    Returns:
        Simple markdown texts generated by VLM.
    """
    import logging
    from PIL import Image as PILImage

    callback = callback or (lambda prog, msg: None)

    img = binary
    txt = ""

    try:
        # Handle different input types
        if isinstance(img, bytes):
            # If it's already bytes, use it directly
            img_bytes = img
        elif isinstance(img, io.BytesIO):
            # If it's BytesIO, read the bytes
            img.seek(0)
            img_bytes = img.read()
        elif isinstance(img, PILImage.Image):
            # Handle PIL Image explicitly
            img_binary = io.BytesIO()
            # Convert to RGB if needed
            if img.mode not in ("RGB", "L"):
                img = img.convert("RGB")
            img.save(img_binary, format="JPEG")
            img_binary.seek(0)
            img_bytes = img_binary.read()
        else:
            # Try to handle as PIL-like object
            img_binary = io.BytesIO()
            # Try to convert to RGB if needed
            if hasattr(img, "mode") and img.mode not in ("RGB", "L"):
                img = img.convert("RGB")
            img.save(img_binary, format="JPEG")
            img_binary.seek(0)
            img_bytes = img_binary.read()

        ans = clean_markdown_block(vision_model.describe_with_prompt(img_bytes, prompt))

        txt += "\n" + ans

        return txt

    except Exception as e:
        import traceback

        error_msg = f"Error in vision_llm_chunk: {str(e)}\nType of input: {type(img)}"
        if hasattr(img, "__class__"):
            error_msg += f"\nClass: {img.__class__}"
        logging.error(error_msg)
        logging.error(traceback.format_exc())
        callback(-1, str(e))

    return ""
